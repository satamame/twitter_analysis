# Plans

## やってみたいこと

- トピックモデルの作成
- トピックモデルを使った、ツイッターユーザの分類

## 計画と成果
---
### 事前準備

プロジェクト開始前に、以下の準備をしました。
- Twitter API へのアクセス
- Tweepy の使い方の確認
- MongoDB のインストール
- PyMongo の使い方の確認
- NoSQLBooster for MongoDB のインストール
- 検索したツイートを MongoDB に保存する実験
- 保存したツイートのユーザ情報だけを抜き出す実験

---
### 05-18 (土)

予定
- どれくらいのツイートを取得できるか実験する。

成果
- 一度に1,200程度のツイートを取得できるようです。

---
### 05-19 (日)

予定
- サンプルツイートを10,000件、DB に保存する。
- サンプルツイートからユーザ情報を抜き出して DB に保存する。
- 今までにやった事などを、一旦ドキュメントにまとめる。
- ラップトップにも開発環境を整える。

成果
- 11,231件のツイートを DB に保存しました。
- 10,793件のユーザ情報を DB に保存しました。

---
### 05-20 (月)

予定
- retrieve_usr_tweets.py を、中断・再開できるようにする。

成果
- retrieve_usr_tweets.py で、ユーザにサンプルとして選ばれたというフラグと、取得したツイート数を記録するようにしました。
- これにより、スクリプトが中断しても、追加でユーザごとのツイートを取得できます。

---
### 05-21 (火)

予定
- gensim の勉強を開始。

成果
- ラップトップに DB のデータを移行することが出来ませんでした（要調査）。
- Janome, scikit-learn, gensim をインストールしました。
- gensim の勉強はできませんでした。

---
### 05-22 (水)

予定
- なし

成果
- DB に保存したツイートを Janome で形態素解析することは可能になりました。
- 名詞, 動詞, 形容詞, 形容動詞の基本形を取るようにしたが、特徴として良くないものも拾ってしまっているので、何とかしたいです。
- 単語列にしたら、一旦 DB に戻すようにする予定。

---
### 05-23 (木)

予定
- Twitter API からのエラーを正しく処理する。

成果
- Rate limit error なら15分待ち、それ以外なら処理を中断するようにしました。

---
### 05-25 (土)

予定
- 形態素解析したツイート本文を DB に保存する。

成果
- 急用のため作業できませんでした。

---
### 05-26 (日)

予定
- 形態素解析したツイート本文を DB に保存する。
- gensim で、最低限のモデルを作ってみる。

成果
- 形態素解析したツイート本文を DB に保存する仕組みを作りました。
- gensim で、特徴語辞書を作るところまでできました。
- モデルは作れませんでした。

---
### 05-27 (月)

予定
- モデルを作るところまで行きたかった。

成果
- ストップワードを正規表現で定義できるようにしました。
- 特徴語辞書をファイルに保存するところまでやりました。

---
### 05-28 (火)

予定
- モデルを作るところまでやりたいです。

成果
- 辞書を保存するスクリプトで、同時にコーパスも保存するようにしました。
- 辞書とコーパスを基に LDA 分類モデルを作って保存するスクリプトができました。
    - モデルを使った分類やモデルの評価はまだやってません。

---
### 06-01 (土)

予定
- 辞書とコーパスを作る際、どのツイートを使ったかの記録を残す。
- [Corpus Streaming – One Document at a Time](
    https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time)
    を参考に、辞書、コーパス、モデルの作成を少ないメモリで出来るようにする。

成果
- 予定に書いた2件はできませんでした。
- サンプルツイートを一度にたくさん取得できるようにし、10万件を DB に保存しました。
- サンプルツイートからユーザ情報だけ抜き出す処理を、少ないメモリで出来るよう改善しました。

---
### 06-02 (日)

予定
- 辞書とコーパスを作る際、どのツイートを使ったかの記録を残す。
- [Corpus Streaming – One Document at a Time](
    https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time)
    を参考に、辞書、コーパス、モデルの作成を少ないメモリで出来るようにする。

成果
- ユーザごとのツイートのコレクションが大きくなったのでカウントするためのクエリを調べました。
- サンプルツイートの Collection に、コーパスに使ったというフラグをつけました。
- 辞書、コーパス、モデルの作成を少ないメモリで出来るようにしました。

---
### 06-04 (火)

予定
- DB のツイートに、訓練データとして使うか、テストデータ（予測用）として使うかのマークをつけられるようにしたいです。
- コーパスを作るスクリプトと分類をするスクリプトは、そのマークを使ってデータを取り出すようにします。

成果
- DB のツイートに、訓練データとして使うマークをつけるスクリプトを作りました。
- 辞書とコーパスを作るスクリプトで、そのマークを見るようにしました。
- モデル作成時、ファイルからコーパスを読み出すのにイテレータオブジェクトを使うようにしました。

---
### 06-08 (土)

予定
- まずは訓練データとして使ったツイートが、どのように分類されるか見れるようにします。

成果
- 人が見て分かるようなテキストにはなってませんが、各ツイートのトピック構成比が数値で分かるようになりました。

---
### 06-09 (日)

予定
- 訓練データの分類結果であるトピック構成比を元に、「何％の確率で何番のトピックである」という情報を、テキストとともにファイルに出力します。

成果
- 最初、分類しながらファイルに出力していたのですが、確率順にソートしたかったので、分類結果を一旦 DB 上に記録して、DB を読みながらファイルに書き出すようにしました。
- 訓練データとテストデータを分けて、それぞれ分類結果を目で見て確認できるようになりました。

---
### 06-10 (月)

予定
- ストップワードをクラスの外で定義するようにする。

成果
- ストップワードを stop_words.py というファイルで定義するようにして、辞書や分類結果から特徴っぽくない言葉を追加しました。

---
### 06-11 (火)

予定
- 形態素解析する前に、メンションや URL を取り除くようにします。

成果
- 形態素解析する前に、メンションや URL を取り除くようにしました。
- ラップトップでも１万件くらいの学習と予測ができるとわかりました。

---
### 06-16 (日)

予定
- 作成したモデルのトピック間の距離を測れるようにします。
- チューニングまでする予定でしたが、もう少し計画を具体化してからにします。

成果
- トピック間の Kullback–Leibler divergence を表示するスクリプトを書きました。
- トピックごとの Coherence を表示するスクリプトを書きました。
- モデルにバージョン番号をつけて、パラメタと評価を保存できるようにしました。

---
### 06-18 (火)

予定
- パラメタやストップワードを変えて、チューニングの効果を見ます。

成果
- マルチコア対応のモデルをやめて、学習時に指定する alpha というパラメタを symmetric -> auto に変えたところ、KL-divergence が 0.8 -> 4.8 へと大きく変化しました。

---
### 06-19 (水)

予定
- Bow 表現を、名詞（代名詞以外）のみにしてみて、効果を見ます。

成果
- KL-divergence が少し上がったが、Coherence は下がった。

---
### 06-20 (木)

予定
- サンプルから、重複するツイートを削除して、分類結果を見ます。

成果
- 重複するツイートを削除する方法として、新たに DB に Collection を作ることにしました。
- DB の処理が重かったのか、途中で止まってしまいました。

---
### 06-21 (金)

予定
- サンプルから、重複するツイートを削除して、分類結果を見ます (昨日のつづき)。

成果
- DB の処理が止まらないようにして、重複のないツイートの Collection を作りました。
    - 重複の削除だけでは、大した効果は得られませんでした。
- no_below を 20 -> 100 に、no_above を 0.2 -> 0.1 にして、高頻度/低頻度の単語を辞書に使わないようにしました。
    - Cogerence は少し下がり、KL-divergence は 5.0 -> 6.0 に上がりました。

---
### 06-22 (土)

予定
- 募集・告知系のツイートの占める割合が多いので、これらが１～２個のトピックにまとまるようにしたい。
- トピック数を半分（4個）に減らしてモデルを作ってみます。

成果
- トピック数を４個にした結果、今までと比べて人間が見てもトピックっぽいものが見えるようになりｍした。
- それぞれ、以下のように定義します。
    1. 紹介/告知系
    1. 意見/感想系
    1. 出会い/アダルト系
    1. 譲渡/交換系
- きれいに分かれているように見えますが、KL-divergence は下がりました。

予定2
- 未知のデータを分類してみて、上で定義したような分類になっているか、人間が見て確認します。

成果2
- 未知のデータの分類結果も、各トピックに訓練データの時と同じようなツイートが入っていました。

予定3
- ユーザ1000人分のツイートを DB に取ってあるので、StreamWords.label_topics メソッドを使ってそれらを分類し、topic_id と topic_prob をセットする。
- その前に、形態素解析をしないといけない。
- 形態素解析に時間がかかるので、とりあえず100人分でやる。

成果3
- 100人分を形態素解析する時間がなかったので、73,000 件 (ユーザ46人分) で中止。これを使って次のステップへ。

---
### 06-23 (日)

予定
- ユーザごとのトピック構成比をリストにして、k-means でグループ分けします。
- 各グループのグラフを重ね合わせる事でトピック構成比のばらつきを見て、ばらつきが少なくなるようなグループ数があるか、試行錯誤してみます。

成果
- 73,000 件を分類する時間がありませんでした。
