# Plans

## やってみたいこと

- トピックモデルの作成
- トピックモデルを使った、ツイッターユーザの分類

## 計画と成果
---
### 事前準備

プロジェクト開始前に、以下の準備をしました。
- Twitter API へのアクセス
- Tweepy の使い方の確認
- MongoDB のインストール
- PyMongo の使い方の確認
- NoSQLBooster for MongoDB のインストール
- 検索したツイートを MongoDB に保存する実験
- 保存したツイートのユーザ情報だけを抜き出す実験

---
### 05-18 (土)

予定
- どれくらいのツイートを取得できるか実験する。

成果
- 一度に1,200程度のツイートを取得できるようです。

---
### 05-19 (日)

予定
- サンプルツイートを10,000件、DB に保存する。
- サンプルツイートからユーザ情報を抜き出して DB に保存する。
- 今までにやった事などを、一旦ドキュメントにまとめる。
- ラップトップにも開発環境を整える。

成果
- 11,231件のツイートを DB に保存しました。
- 10,793件のユーザ情報を DB に保存しました。

---
### 05-20 (月)

予定
- retrieve_usr_tweets.py を、中断・再開できるようにする。

成果
- retrieve_usr_tweets.py で、ユーザにサンプルとして選ばれたというフラグと、取得したツイート数を記録するようにしました。
- これにより、スクリプトが中断しても、追加でユーザごとのツイートを取得できます。

---
### 05-21 (火)

予定
- gensim の勉強を開始。

成果
- ラップトップに DB のデータを移行することが出来ませんでした（要調査）。
- Janome, scikit-learn, gensim をインストールしました。
- gensim の勉強はできませんでした。

---
### 05-22 (水)

予定
- なし

成果
- DB に保存したツイートを Janome で形態素解析することは可能になりました。
- 名詞, 動詞, 形容詞, 形容動詞の基本形を取るようにしたが、特徴として良くないものも拾ってしまっているので、何とかしたいです。
- 単語列にしたら、一旦 DB に戻すようにする予定。

---
### 05-23 (木)

予定
- Twitter API からのエラーを正しく処理する。

成果
- Rate limit error なら15分待ち、それ以外なら処理を中断するようにしました。

---
### 05-25 (土)

予定
- 形態素解析したツイート本文を DB に保存する。

成果
- 急用のため作業できませんでした。

---
### 05-26 (日)

予定
- 形態素解析したツイート本文を DB に保存する。
- gensim で、最低限のモデルを作ってみる。

成果
- 形態素解析したツイート本文を DB に保存する仕組みを作りました。
- gensim で、特徴語辞書を作るところまでできました。
- モデルは作れませんでした。

---
### 05-27 (月)

予定
- モデルを作るところまで行きたかった。

成果
- ストップワードを正規表現で定義できるようにしました。
- 特徴語辞書をファイルに保存するところまでやりました。

---
### 05-28 (火)

予定
- モデルを作るところまでやりたいです。

成果
- 辞書を保存するスクリプトで、同時にコーパスも保存するようにしました。
- 辞書とコーパスを基に LDA 分類モデルを作って保存するスクリプトができました。
    - モデルを使った分類やモデルの評価はまだやってません。

---
### 06-01 (土)

予定
- 辞書とコーパスを作る際、どのツイートを使ったかの記録を残す。
- [Corpus Streaming – One Document at a Time](
    https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time)
    を参考に、辞書、コーパス、モデルの作成を少ないメモリで出来るようにする。

成果
- 予定に書いた2件はできませんでした。
- サンプルツイートを一度にたくさん取得できるようにし、10万件を DB に保存しました。
- サンプルツイートからユーザ情報だけ抜き出す処理を、少ないメモリで出来るよう改善しました。

---
### 06-02 (日)

予定
- 辞書とコーパスを作る際、どのツイートを使ったかの記録を残す。
- [Corpus Streaming – One Document at a Time](
    https://radimrehurek.com/gensim/tut1.html#corpus-streaming-one-document-at-a-time)
    を参考に、辞書、コーパス、モデルの作成を少ないメモリで出来るようにする。

成果
- ユーザごとのツイートのコレクションが大きくなったのでカウントするためのクエリを調べました。
- サンプルツイートの Collection に、コーパスに使ったというフラグをつけました。
- 辞書、コーパス、モデルの作成を少ないメモリで出来るようにしました。

---
### 06-04 (火)

予定
- DB のツイートに、訓練データとして使うか、テストデータ（予測用）として使うかのマークをつけられるようにしたいです。
- コーパスを作るスクリプトと分類をするスクリプトは、そのマークを使ってデータを取り出すようにします。

成果
- DB のツイートに、訓練データとして使うマークをつけるスクリプトを作りました。
- 辞書とコーパスを作るスクリプトで、そのマークを見るようにしました。
- モデル作成時、ファイルからコーパスを読み出すのにイテレータオブジェクトを使うようにしました。

---
### 06-08 (土)

予定
- まずは訓練データとして使ったツイートが、どのように分類されるか見れるようにします。

成果
- 人が見て分かるようなテキストにはなってませんが、各ツイートのトピック構成比が数値で分かるようになりました。

---
### 06-09 (日)

予定
- 訓練データの分類結果であるトピック構成比を元に、「何％の確率で何番のトピックである」という情報を、テキストとともにファイルに出力します。

成果
- 最初、分類しながらファイルに出力していたのですが、確率順にソートしたかったので、分類結果を一旦 DB 上に記録して、DB を読みながらファイルに書き出すようにしました。
- 訓練データとテストデータを分けて、それぞれ分類結果を目で見て確認できるようになりました。

---
### 06-10 (月)

予定
- ストップワードをクラスの外で定義するようにする。

成果
- ストップワードを stop_words.py というファイルで定義するようにして、辞書や分類結果から特徴っぽくない言葉を追加しました。

---
### 06-11 (火)

予定
- 形態素解析する前に、メンションや URL を取り除くようにします。

成果
- 形態素解析する前に、メンションや URL を取り除くようにしました。
- ラップトップでも１万件くらいの学習と予測ができるとわかりました。
